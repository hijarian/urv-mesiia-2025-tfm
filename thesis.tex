\documentclass[12pt, a4paper]{article}
%\usepackage[utf8]{inputenc} 
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{xcolor}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\usepackage[backend=biber]{biblatex}
\usepackage{csquotes} % smart quotes + tight integration with biblatex
% Custom bibliography file
\addbibresource{library.bib} 

\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

% Must compile the document with -shell-escape
% Must compile the document in an environment where the `pygments` library is installed in Python
%\usepackage{minted}

% Source code highlighting BEGIN
\usepackage{listings}
\lstset{basicstyle=\small,defaultdialect=[11]c++,frame=lines,numbers=left,numberstyle=\tiny,tabsize=2}
% YAML highlighting hack BEGIN
% Courtesy of Stackoverflow: https://tex.stackexchange.com/a/152856/179871

\newcommand\YAMLcolonstyle{\color{red}\mdseries}
\newcommand\YAMLkeystyle{\color{black}\bfseries\smallfont}
\newcommand\YAMLvaluestyle{\color{blue}\mdseries}

\makeatletter

% here is a macro expanding to the name of the language
% (handy if you decide to change it further down the road)
\newcommand\language@yaml{yaml}

\expandafter\expandafter\expandafter\lstdefinelanguage
\expandafter{\language@yaml}
{
	keywords={true,false,null,y,n},
	keywordstyle=\color{darkgray}\bfseries,
	basicstyle=\YAMLkeystyle,                                 % assuming a key comes first
	sensitive=false,
	comment=[l]{\#},
	morecomment=[s]{/*}{*/},
	commentstyle=\color{purple}\ttfamily,
	stringstyle=\YAMLvaluestyle\ttfamily,
	moredelim=[l][\color{orange}]{\&},
	moredelim=[l][\color{magenta}]{*},
	moredelim=**[il][\YAMLcolonstyle{:}\YAMLvaluestyle]{:},   % switch to value style at :
	morestring=[b]',
	morestring=[b]",
	literate =    {---}{{\ProcessThreeDashes}}3
	{>}{{\textcolor{red}\textgreater}}1     
	{|}{{\textcolor{red}\textbar}}1 
	{\ -\ }{{\mdseries\ -\ }}3,
}

% switch to key style at EOL
\lst@AddToHook{EveryLine}{\ifx\lst@language\language@yaml\YAMLkeystyle\fi}
\makeatother

\newcommand\ProcessThreeDashes{\llap{\color{cyan}\mdseries-{-}-}}
% YAML highlighting hack END
% Source code highlighting END


% Title of your project
\title{Fuzzy-Genetic Hybrid Models for Large-Horizon Deterministic Planning}

% Name of deliverable
\newcommand{\deliverableName}{Master's Thesis}

% Group names(s)
\author{Mark Safronov}

% Group number
\newcommand{\groupNumber}{A_2024-25_105200}

% Any comments for us
\newcommand{\comments}{Comments for teachers of the course}

% Web address for the project (if any)
\newcommand{\homepage}{\url{https://campusvirtual.urv.cat/course/view.php?id=105200}}

% Date for title page, default is today and 
\date{\today}


\makeatletter{}

\begin{document}
	
	\input{./title.tex}
	
	\tableofcontents
	
	\section{Introduction}
	
	A deterministic planning problem for a stateful agent is a classical and generic problem applicable to a variety of real-world problems.
	However, it is also prone to a combinatorial explosion when the range of the actions to choose and the planning horizon (length of the sequence of actions to find) become sufficiently large.
	Works like~\cite{WOS:001308319900003} cover both the real-world applications and a significance of the ``curse of dimensionality''.
	
	In this work we will discuss an alternative approach of solving deterministic planning problems for stateful agents with large planning horizons and large action spaces, using a hybrid fuzzy-genetic system which relies on a domain knowledge to reduce the dimensionality.
	
	In this introduction section we will set up the context of a problem to be solved and briefly cover the most obvious approaches to its solution.
	Readers which don't require such an introduction can proceed directly to the section \ref{problem-statement}.
	
	\subsection{A game solver as a planning problem}

	In 1993 in Japan Studio Gainax released a computer game called \textit{Princess Maker 2}.
	The game essentially defined a so-called ``life simulation'' genre.
	The player takes the role of a guardian of a young girl, making decisions that affect her upbringing and future.
	\textit{Princess Maker 2} is a narrative-heavy game, with a diverse set of gameplay elements depending on player's choices, but under the plot and all the other art elements which normally constitute a computer game, lies very formal and routine core gameplay loop:
	
	\begin{enumerate}
		\item The player estimates the current state of the girl;
		\item The player chooses the day-to-day schedule for the girl;
		\item The girl ``performs'' the scheduled actions;
		\item The game changes the state of the girl according to the actions performed.
	\end{enumerate}
	
	Story-wise, the game ends after ten years of upbringing the girl.
	At this point the game evaluates the state reached by the girl and tells the final ``fate'' she got as the result.
	The title of the game is the hint on the ultimate goal, but depending on the final characteristics of the girl (and some special story-dependent flags we ignore in this work) game can end in more than twenty different endings.
		
	This is essentially a stateful agent planning its behavior according to the predefined goal to be reached.
	
	Thus, we organically come to a planning problem, and a question arises: can we solve it? Given the desired ending, can we deduce the total ``schedule'', a full list of choices to make to get to this ending, automatically and in one go? This question lies in the core of this work.
	
	Solving the full Princess Maker 2 game is completely out of scope of this work.
	The game contains a set of auxiliary gameplay mechanics which significantly influence the evolution of a character, and a hidden storyline-dependent flags which are necessary to reach some of the endings.
	These elements are not fit for modelling as a planning problem.
	
	So, we will abstract the origin game into the most simplistic model, namely:
	
	\begin{enumerate}
		\item only the numerical attributes of a character are considered for an ending state (we ignore story-related boolean flags)
		\item each action has a completely determined effect on character attributes (in the original game actions have randomized effects from a range of values)
		\item actions are the only way to change the character state (we ignore all other gameplay mechanics except the week-by-week schedule of actions)
	\end{enumerate}

	As the result of this simplification, we are left over with a deterministic planning problem, which nevertheless has a significant scale, because in the Princess Maker 2 game there are:
	
	\begin{enumerate}
		\item 50 numerical attributes of a character
		\item 25 possible actions to schedule
		\item the end state is evaluated and compared to the goal after 360 action steps.
	\end{enumerate}

	This presents a significant challenge to classic algorithms.
	
	At the same time, because this problem is so generic, any progress in finding an efficient solution to it is beneficial to multiple areas.
	
	\subsection{Existing methods assessment}\label{introduction:assessment}

	There are two approaches which are deceptively obvious choices to solve this problem, namely, automatic planning theory~\cite{fikes1971strips} and reinforcement learning~\cite{sutton2018reinforcement}.
	
  \subsubsection{Automatic planning theory}
  
  To quote the foundational STRIPS paper:
  
  \blockcquote[p.~190]{fikes1971strips}
  {The task of the problem solver is to find some composition of operators that transforms a given initial world model into one that satisfies some stated goal condition.}
  
  This sounds exactly like the description of the solver we want to get.
  
  	Stateful agent can be modeled using any of the modern automatic planners which support numeric fluents, for example, ENHSP~\cite{enhsp::scala2020subgoaling}~\cite{enhsp::Scala2016IntervalBasedRF}.
	
	This is proven to work on small-scale problems: the code repository of the ENHSP-20 solver~\footnote{https://github.com/hstairs/enhsp/tree/enhsp-20} has benchmarks with essentially 40 numeric parameters and 10 action choices.
	
	But the very nature of the automatic planners, namely, the intent to search for the shortest schedule reaching the goal, contradicts the problem we want to solve.
	The \textit{Princess Maker} problem dictates delayed evaluation of the goal state, we must execute a set amount of actions, and stay at the goal state at that time.
	More than that, as we define more rigorously in the section \ref{problem-statement}, there are no ``no-operation'' actions, we cannot ``do nothing'' to pad the actions sequence, and every action changes the state.
	
	So, while using the automatic planners almost perfectly matches our needs, this is not the goal of this work.
	In addition to that, the problem of scale stays unexplored, namely, the specific Princess Maker problem is just an example of a large-scale planning problem which we want to explore in this work.
	
	Though, there exist works which directly tackle the problem of scale in the currently existing automatic planners, but using different approaches, namely, Linear Temporal Logic~\cite{LU2025121666}.
	They don't focus specifically on numeric fluents, though.

  \subsubsection{Reinforcement learning}
	Second, we can try applying the Reinforcement Learning~\cite{sutton2018reinforcement} to this problem.
	The choice of this optimization method is deceptively obvious, because at a glance, the problem looks like a perfect match for it.
	We have an agent, which has a state, and this agent can perform actions which change the state.
	Ultimately we want the agent to reach the goal state which will give it the best reward.
	
	The issue of scale is the main obstacle in this case.

	The origin problem of solving Princess Maker 2, described in the previous subsection,  assumes 25 actions over a state space of 50 numeric characteristics each one having values between 0 and 500.
	Just enumerating the possible states of the agent caused by these actions in the state space of such a size is an intractable problem, which we will rigorously show in \ref{methodology}.
	Just the rewards table for this has a size of $25 \times 500^{50}$, which is already practically intractable.
	
	Moreover, the most important problem is the length of the process.
	Following the base example of \textit{Princess Maker 2} gameplay, player makes 3 choices per virtual ``month'', and the game spans 10 ``years'', so the search space is a tree $3 \times 12 \times 10 = 360$ levels deep.
	
	\section{Formal problem statement}\label{problem-statement}

	As follows from the section~\ref{introduction:assessment}, formally we have a choice of whether to treat this as a planning problem or a control problem.
	Despite Reinforcement learning not being fit for our cause, we will still approach our origin problem as a control problem, as it allows us to formally introduce our solution approach later in~\ref{section::solution}.

	\subsection{Actor behavior as a control problem}

	Assuming we have a character described as a set of numeric characteristics
	
	\begin{equation}
		\mathbf{x} \in \mathbb{Z}^n
	\end{equation}
	
	we have a set of possible actions
	
	\begin{equation}
		A = \{a_1, a_2,\ldots, a_m\}
	\end{equation}
	
	which collectively form a transfer function
	
	\begin{equation}\label{definitions:transfer-function}
		f(\mathbf{x}, a) = \mathbf{x}'
	\end{equation}

	To describe the desired outcome, we first declare a fitness function mapping the state to a numerical value:
	
	\begin{equation}
		\Phi : \mathbf{x}' \rightarrow \mathbb{R}
	\end{equation} 
	
	a goal fitness value 
	
	\begin{equation}
		\mathbf{G} \in \mathbb{R}
	\end{equation}
	
	and a planning horizon
	
	\begin{equation}
		T \in \mathbb{Z}
	\end{equation}
	
	We want to get an ordered actions sequence of length $T$ which will lead $\mathbf{x}$ to some $\mathbf{x}^*$:
	
	\begin{eqnarray}\label{definitions:fold}
		\mathbf{a} \in A^T, x_o = \mathbf{x}: \bigodot_{i=1}^{T} f(x, a_i) = \mathbf{x}^*
	\end{eqnarray}
	
	(where $\bigodot$ is a fold operator)
	
	such as:
	
	\begin{equation}
		\Phi(\mathbf{x}^*) > \mathbf{G}
	\end{equation}
	
	The transfer function $f$ is assumed to be completely determined, and the whole process being non-stochastic. This is a significant restriction which cannot be lifted for the proposed solution to work.

	\subsection{Dimensionality explosion stemming from the original context}

	We assume a fixed-length trajectory of $𝑇$ actions, each of which transforms the state of the system according to a known deterministic transfer function~(\ref{definitions:transfer-function}).

	This means two restrictions:
	
	\begin{enumerate}
		\item No-operation actions are prohibited, each step must result in a meaningful state transformation, reflecting the irreversible nature of time.
		\item Goal state must still be in effect at the step $T$.
	\end{enumerate}

	While the agent cannot avoid taking actions — and hence cannot avoid changes to the system — it is allowed to evaluate its progress toward the goal at every intermediate state.
	In this sense, the problem is not a pure planning task but an episode-based control problem with delayed evaluation.
	
	In this work we'll focus specifically on the cases which lead to combinatorial explosion for classical solutions, that is, when we have sufficiently large amount of characteristics, actions to choose from and most importantly, very large planning horizon:
	
	\begin{eqnarray}
		n > 50 \\
		m > 20 \\
		T > 360
	\end{eqnarray}
	
	The origin \textit{Princess Maker} problem is the lower edge of the cases we are interested in.

	With these restrictions in place, a need in an heuristic arises to perform efficient search in the state space, as its size becomes unrealistically large.
	
	\section{Proposed solution approach}\label{section::solution}

Classical reinforcement learning methods become intractable in this domain due to the high dimensionality of the state space, large action set, and long planning horizon. Moreover, the inability to halt or take neutral actions further exacerbates the combinatorial explosion of the trajectory space.

To address this, we introduce a heuristic dimensionality reduction via the concept of inclinations — latent behavioral parameters — and model the behavior policy as a fuzzy controller which maps the current state and inclinations to a concrete action.

This parametrization constrains the space of possible behaviors, making the optimization tractable. Instead of learning or searching over action sequences directly, we perform optimization in the significantly smaller space of inclinations, evaluating the final outcome after $T$ steps. The resulting problem becomes an offline, black-box control task — suitable for evolutionary algorithms, rather than classical RL methods.

	\subsection{Using domain knowledge to reduce dimensionality}

	In this work we evaluate an approach which is defined as follows.
	
	Let's assume that we can segment the set of possible actions to clusters with the following particularities:
	
	\begin{enumerate}
		\item actions in the same cluster lead to ``similar'' changes in the character state $\mathbf{x}$.
		\item the cluster as a whole can be described symbolically
	\end{enumerate}
	
	In this case we can synthesize a set of numeric characteristics which we'll call ``inclinations'':
	
	\begin{eqnarray}
		\mathbf{I} \in \mathbb{Z}^q\\
		q << n \label{q<<n}
	\end{eqnarray}

	From this, we can define a set of fuzzy rules\cite{ray2014softcomputing} mapping the inclinations to action choices:

	\begin{enumerate}
		\item if an inclination $I_i$ has a fuzzy value $V_I$,
		\item and the current state $\mathbf{x}$ has fuzzy values $V^x_i$
		\item then $P_a$, the priority of an action $a$, is a fuzzy set $V_A$.
	\end{enumerate}
	
	After the defuzzification of all the inferred fuzzy values $P_a$ we select an action with the highest priority.
	
	The selection and design of fuzzy rules is a critical aspect of this approach.
	In this thesis, the fuzzy rule base is constructed manually, leveraging domain knowledge to define the mapping from inclinations to action priorities.
	Future research may investigate automated methods for generating fuzzy rules, such as clustering or machine learning techniques, to further improve scalability and reduce manual effort.

	While it is theoretically possible to define fuzzy rules that map every possible inclination vector $\mathbf{I}$ or even every state $\mathbf{x}$ to action priorities, such exhaustive rule sets would quickly become infeasible due to combinatorial growth.
	This reinforces the importance of dimensionality reduction and clustering in making the fuzzy-genetic approach tractable for high-dimensional planning problems.

	The assumption which we explore among others in this work is the practical possibility to write a coherent set of fuzzy rules which will be clustered around the clusters of actions, and each inclination will tend to map to its own cluster of actions.
	
	Now, using such a fuzzy controller $\xi(I, \mathbf{x})$ we can construct the goal function:
	
	\begin{equation}\label{definitions:goal-function}
		g(I, \mathbf{x}) = \bigodot_{i=1}^{T} f(x, \xi(I, x_i))
	\end{equation}
	
	the above formula being subject to improvements in expressiveness,
	the main point of which being the fuzzy controller $\xi(I, x_i)$ selecting the action to perform on the step $i$ according to the inclinations and (ideally) the current state $x_i$.
	
	The argument $\mathbf{x}$ is essentially a constant for both~(\ref{definitions:fold}) and~(\ref{definitions:goal-function}).
	As the transfer function $f$ is non-stochastic, $\mathbf{I}$ uniquely maps to the actions sequence $\mathbf{a}$.
	Thus, given (\ref{q<<n}), we effectively performed dimensionality reduction on the original problem.
		
	We can find $\arg \max(g)$ now using an appropriate optimization method.
	For this work, because of a strong biosocial analogies a genetic algorithm\cite{mitchell1999geneticalgorithms} was chosen,
	with the vector of inclinations $\mathbf{I}$ as a chromosome.
	
	\subsection{Hypothesis}
	The hypothesis explored in this work is that the combination of assumptions described above constructs an heuristic which allows finding a locally optimal solution in a polynomial time.
	
	\subsection{Objectives}
	The objective of this thesis is to investigate whether the stated hypothesis is true.
	That is, whether a fuzzy-genetic heuristic can effectively solve high-dimensional deterministic planning problems through dimensionality reduction and symbolic reasoning.
	
	In particular, we aim to:
	\begin{enumerate}
		\item Formalize the problem as an optimization task.
		\item Implement a working solver.
		\item Evaluate the performance of the solver on a set of test cases of increasing complexity.
		\item Analyze the results to draw conclusions about the effectiveness of the approach.
	\end{enumerate}

	\section{Methodology}\label{methodology}

  In this section we will discuss the theory which this work is build upon, namely, fuzzy logic~\cite{ray2014softcomputing} and evolutionary algorithms~\cite{mitchell1999geneticalgorithms}.

	\subsection{Encoding domain knowledge of actions as a fuzzy controller}

  Normally the Fuzzy logic is being explained from the fuzzy set theory by L. Zadeh~\cite{zadeh1965fuzzy}, but for this particular work the most important part of the fuzzy logic is the fuzzy rules for the fuzzy controller so it's more beneficial to start with them.

  In the scope of the FL it is possible to express the domain knowledge in the form of symbolic rules, with the general form as follows:

  \begin{verbatim}
    If (input variable A) has a (fuzzy value Fa) 
    	then (output variable B) has a (fuzzy value Fb)
  \end{verbatim}

  For example, for our particular problem and solution method:

  \begin{verbatim}
    If InclinationAggressiveness is High 
    	then DuelingClassesPriority is High
  \end{verbatim}

  This rules format depends on the concept of the \textbf{Fuzzy Variable}, which is a combination of four major parts:

  \begin{enumerate}
    \item Name
    \item Range of ``strict'' values
    \item ``strict'' value itself
    \item Set of fuzzy sets describing the possible fuzzy values of this variable
  \end{enumerate}

  The concept of Fuzzy Variable, in turn, depends on the concept of a \textbf{fuzzy value}, which is a combination of two major parts:

  \begin{enumerate}
    \item Name
    \item Membership function
  \end{enumerate}

  Where the \textbf{membership function} is a continuous function mapping the input ``strict'' values to real numbers between 0 and 1.
  The \textit{membership function} of a fuzzy value describes the \textit{measure of belonging} of the current ``strict'' value of the variable to the given symbolic \textbf{term},
  for example, ``high'', ``low'' and such.
  Because of the \textit{terms} being literally words from a natural language, fuzzy variable is also called a \textbf{linguistic variable}.

  Let's give \textbf{an example}.
  Assume the following fuzzy variable:

  \begin{eqnarray}
    S = \left(N, R, V, T\right)\\
    N = \textrm{``Strength''}\\
    R = \mathbb{Z} \in [0, 100]\\
    V \in R\\
    T = \left(\left(``Low'', f_l\right), \left(``Acceptable'', f_a\right), \left(``High'', f_h\right)\right)
  \end{eqnarray}

  It specifies three \textit{fuzzy terms} for the numeric property ``Strength'', which can have integer ``strict'' values from 0 to 100.
  Thus, when we measure this property and provide a strict value for ``Strength'', we can determine the values of \textit{membership functions} of its three \textit{fuzzy terms}.
  For example, if
    $V = 72$
  then
    $T = \left(\left(``Low'', f_l(72) \right), \left(``Acceptable'', f_a(72) \right), \left(``High'', f_h(72)\right)\right)$

  Which should be interpreted as ``Strength'' of 72 being at the same time $f_l(72)$ ``Low'', $f_a(72)$ ``Acceptable'' and $f_h(72)$ ``High''.

  The process of calculating the values of membership functions for all the terms of a linguistic variable given its strict value is called \textbf{fuzzification} of this value.

  The main point of the fuzzification, which we exploit in our method and which is at the core of the fuzzy control theory, is that we get the formal mechanism of transforming numeric values to domain-specific inexact vocabulary.

  Fuzzy logic provides the reverse process as well.
  It is possible to specify the values of the membership functions of all the terms in $T$ of the fuzzy variable, and from them calculate the ``strict'' value $V$.
  This process is called \textbf{defuzzification} of the linguistic variable.

  Continuing the above example, we can start by specifying the fuzzy values of ``Strength'' first, possibly, if we measure it by some inexact vague means:
  $f_l = 0.4$, $f_a = 0.8$, $f_h = 0$.

  Then, depending on the exact shape of the functions $f_l$, $f_a$ and $f_h$ defuzzification gives us a strict value of ``Strength'', say, $42$.

  Given all the above, a \textbf{fuzzy controller} is an algorithm which performs three large steps:

  \begin{enumerate}
    \item Applies fuzzification of the values of all the input variables (antecedents of the fuzzy rules)
    \item Evaluate all the fuzzy rules, obtaining the fuzzy values of the output fuzzy variables
    \item Applies defuzzification to the output fuzzy variables, obtaining their strict values.
  \end{enumerate}

  The above algorithm is called a \textbf{Mamdani fuzzy controller} and it's the one which we'll use in this work.

  In our system, we're going to have the vector of inclinations and the current state of the specimen as input variables for the controller,
  and have the priorities of actions as the output variables.
  This will allow us to imitate the process of ``decision making'' of the specimen to choose the next action to perform.

	\textbf{The major benefit and the core reason} for the fuzzy controller is the ability to encode the domain knowledge in a limited set of rules which will be formally processed.

	Compared to, for example, some of the reinforcement learning methods, we don't need to specify the full table of rewards for every possible action-state combination.
	It is enough to specify one rule for every available action and the controller will already become fully functional.
	With some configuration of rules it's possible to write even less of them.

	This allows to simplify the implementation of the solver, because one of the main weaknesses of the proposed solution is writing the fuzzy rules by hand.

  \textbf{The second benefit} of using the fuzzy controller for decision making is that it can be applied without major changes to non-deterministic, stochastic environment,
  for example, if the actions would be allowed to make randomized changes to the specimen's state, that is, if the transfer function would not be pure.
  It opens up the possibilities to explore this topic further in the later works.

	\subsection{Control feedback loop as a fitness function}\label{fitness}

	A single trajectory in the action space is explored using the following process.

	\begin{enumerate}
		\item We start with the initial state $\mathbf{x}_0$ and the given set of inclinations $\mathbf{I}^k$
		\item\label{loop:evaluate} We evaluate both $\mathbf{x}_0$ and $\mathbf{I}^k$ with the preconfigured fuzzy controller
		\item The defuzzified output of the controller is the set of priorities for all the  actions. We pick the action with the highest priority. Tiebreaker is the position of the action in the list.
		\item Action is executed and if we haven't made $T$ actions yet we return to the step \ref{loop:evaluate}
		\item After $T$ executed actions we apply the goal conditions predicate $\Phi(\mathbf{x}*)$ and calculate the fitness based on that.
	\end{enumerate}

  It is important to understand that the state of a specimen is a transient value, used only for calculations of the final fitness after $T$ iterations.
  The solution we seek is fully encoded in the inclinations vector $\mathbf{I}$, which stays unchanged for the entirety of the control loop.

	\subsection{Global optimization using an evolutionary algorithm}\label{section::evolutionary}

	Strong biosocial analogies and the configuration of the control loop from~\ref{fitness} suggest us to use the evolutionary algorithms~\cite{song2023rl_ea}~\cite{beyer2002evolution_strategies} for optimization.
	This is what would be used in this work.
	However, in principle, any algorithm which is able to use the concept of a fitness function would be applicable here.

  Evolutionary algorithms can be explained with an example of the so-called Simple Genetic Algorithm~\footnote{https://esa.github.io/pagmo2/docs/cpp/algorithms/sga.html}.

  SGA operates on the set of \textbf{specimens}, each one being a single option in the search space to explore.
  A specimen is classically a list of characters, which is called literally a \textbf{genome}.
  The whole set of specimens is called a \textbf{population}.

  In our case, a specimen would be a list of inclination values.

  Every specimen in a population is evaluated using the \textbf{fitness function}, producing a fitness value.

  Then, a \textbf{selection operator} is applied, choosing a subset of the population.
  For example, our selection operator may be choosing the top 50\% of the population by their fitness value.

  After the selection, we apply the \textbf{crossover operator} to the pairs of selected specimens' genomes.
  The classical crossover operator picks a single place inside both of the genomes and then swaps the resulting halves between them.
  For example, a genome `aaaa000` and a genome `1111bbb` after the crossover at point 5 become `aaaabbb` and `1111000`.

  After the crossover we apply the \textbf{mutation operator} to all of the selected genomes.
  The mutation changes (with some low probability) individual genes in the genomes at random.
  For example, we can have a mutation operator which has 0.01 probability of flipping a gene in the genome from `a` to `b` and \textit{vice versa}.
  Then, we have 0.002 probability of a specimen with a genome `aaabb` turning into `ababb`.

  After the crossover and mutation, we finally apply the \textbf{replacement} which forms the new population for the next generation and the next round of evolution.
  For example, we can use a so-called $\left(\mu + \lambda\right)$-evolution strategy~\cite{schwefel1981numerical}: calculate the fitness for all the new genomes and then pick $s$ ones with the best fitness from both the old genomes and new ones, where $s$ is the target population size.
  The size of the population is being kept constant for the classical genetic algorithms, the role of the replacement operator is specifically to enforce that.

  In the approach described in this work, the fitness function is the control loop described in the previous section~\ref{fitness}.
  The genome of the specimen is the vector of inclinations.
	And due to the choice of the specific library for the implementation of the evolutionary algorithms we have a wide selection of them,
  which means, we can explore different options starting from the Simple Genetic Algorithm and continuing with more complicated options.

	The library Pagmo \cite{Biscani2020} includes a lot of already implemented different evolutionary algorithms apart from the simple genetic algorithm so it enables us easier exploration of possibilities in optimizing the full solver.

	\section{Implementation}

	The technical implementation of the method is performed in C++~\cite{ppp3} using the libraries FuzzyLite~\cite{fl::fuzzylite} and Pagmo~\cite{Biscani2020}

	\subsection{Choice of a C++ language as foundation}

	As the root problem of this work is the problem of scale, it has been decided that we trade comfort of experimentation for pure processing power.

	Contemporary C++, starting with the standard version 20, allows for a very high-level code as readable as a natural language.
  It also has libraries for both the fuzzy logic~\cite{fl::fuzzylite} and evolutionary computations~\cite{Biscani2020} for us to not implement any of them from scratch.

	In talking on choice of the language for the implementation we cannot avoid comparisons with Python, assumed leader and language of choice for scientific experiments.
	As has been stated above, it has been conscious decision to trade the ability to make rapid changes in the code, especially the ability to run convenient machinery like Jupyter notebooks, for the raw processing power.
	This is because the C++20 and later is expressive enough to be as readable as Python sans some of the required syntax boilerplate, and in reality the most painful part of choosing C++ is building the program to be cross-platform, as Python programs are, and doing that with the code which uses third-party libraries is a nontrivial implementation problem.

	\subsection{Fuzzylite library for the fuzzy controller implementation}\label{fuzzy-implementation}

	This work turned out to be more or less an assessment of usefulness of the \texttt{fuzzylite}~\cite{fl::fuzzylite} C++ library in addition to the main goal.
	While being fully open sourced with a non-restrictive license terms, actually adding it to an existing C++ program is a task certainly not feasible for an arbitrary computer scientist not being the seasoned software engineer at the same time.
	Which is a shame, as it offers a straightforward idiomatic API which allows expressing the algorithms in a readable format.

	\texttt{fuzzylite} also provides a domain-specific language for specifying the fuzzy controller, which allows us to describe this part of the algorithm in a language more expressive than the raw C++ function calls.

  On the following code example is a description of a fuzzy variable in the DSL of \texttt{fuzzylite}.

  \begin{lstlisting}[language=yaml]
  InputVariable: PhysicalInclination
    enabled: true
    range: 0 1.000
    lock-range: false
    term: tiny Ramp 0.330 0.000
    term: low Triangle 0.000 0.330 0.670
    term: high Triangle 0.330 0.670 1.000
    term: highest Ramp 0.670 1.000
  \end{lstlisting}

  Base syntax of this DSL is essentially YAML~\footnote{https://yaml.org/}.

  At the first line we specify the name of the variable and whether it will be used as an input or an output for the fuzzy rules.
  Among the properties of the variable we have the numerical range of strict values for it, supplementary flags \texttt{enabled} and \texttt{lock-range} not interesting at this moment
  and several \texttt{term} declarations which are the concise descriptions of all the linguistic terms of the variable.

  In the example only two membership functions are used: \texttt{Ramp} and \texttt{Triangle}, but \texttt{fuzzylite} has around 20 of them predefined at the time of writing this report.
  
  The following line specifies a single term of a fuzzy variable:
  
  \begin{lstlisting}[language=yaml]
  	term: tiny Ramp 0.330 0.000
  \end{lstlisting}
  
  In this line, the word \texttt{tiny} is the symbolic name of the term, which represents the vague description of the value directly from the domain knowledge.
  
  The word \texttt{Ramp} is a keyword selecting the appropriate membership function from among the ones built-in in the fuzzylite library.
  Figure~\ref{fig:left-ramp} displays the plot of this function.
  
  The notation \texttt{0.330 0.000} is an internal trick of the library to indicate the downward slope of the ramp by convention instead of some other method.
  Writing the $x$ values in ascending order would mean that the ramp is increasing instead of decreasing.
  
  \begin{figure}[htbp]
  	\centering
  	\begin{tikzpicture}
  		\begin{axis}[
  			width=0.7\linewidth,
  			axis lines=left,
  			xlabel={$x$},
  			ylabel={$\mu(x)$},
  			xmin=-0.15, xmax=1.05,
  			ymin=-0.02, ymax=1.02,
  			xtick={0,0.3333,1},
  			xticklabels={$0$, $0.33$, $1$},
  			ytick={0,1},
  			clip=false
  			]
  			% Левая плато при x<0, затем линейная рампа до 0.33 и ноль дальше
  			\addplot[very thick] coordinates {
  				(-0.15,1) (0,1) (0.3333333333,0) (1.05,0)
  			};
  			
  			% (опционально) направляющие штриховые линии для 0 и 0.33
  			\addplot[densely dotted] coordinates {(0,0) (0,1)};
  			\addplot[densely dotted] coordinates {(0.3333333333,0) (0.3333333333,1)};
  		\end{axis}
  	\end{tikzpicture}
  	\caption{``Left ramp'' membership function: $\mu(x)=1$ when $x<0$, linearly decreasing on $[0,\,0.33]$, and $\mu(x)=0$ when $x>0.33$.}
  	\label{fig:left-ramp}
  \end{figure}
  
  Given the definitions of all the fuzzy variables, the list of rules of the fuzzy controller is specified in almost the natural language~\footnote{``then'' clauses has been moved to the next lines for the line to fit on the paper, in an actual code the rule is written on a single line without breaks}:

  \begin{lstlisting}[language=yaml]
  RuleBlock: mamdani
    enabled: true
    conjunction: Minimum
    disjunction: Maximum
    implication: AlgebraicProduct
    activation: General
    rule: if PhysicalInclination is low  and MentalInclination is low
    	     then MannersClass is high
    rule: if PhysicalInclination is low  and MentalInclination is low 
    	     then Hunting is low
  \end{lstlisting}

	
  \texttt{RuleBlock} declaration specifies what is the exact variant of fuzzy controller we are going to use, the Mamdani~\cite{fuzzy::Mamdani} one or Sugeno~\cite{fuzzy::Sugeno} one.
  In the scope of this work we will be using Mamdani controllers exclusively.

	\subsection{Pagmo library for evolutionary computations}

	Authors of the Pagmo library designed a very high-level API for the evolutionary computation, which can be completely summarized in the following code snippet:
	
	\begin{lstlisting}[language=c++]
// declare the problem to solve
pagmo::problem prob(pm_problem{});

// declare the algorithm to use
pagmo::algorithm algo(pagmo::sade(100));

// declare the population to use
pagmo::archipelago archi(16u, algo, prob, 20u);

// work
archi.evolve(10);
archi.wait_check();

for (const auto& isl : archi)
{
	const auto& champion = isl.get_population().champion_x();
	// for example, print the champion.
}
	\end{lstlisting}

	Pagmo library includes a large amount of already implemented algorithms which brings us two benefits:
	
	\begin{enumerate}
		\item there's no need to implement them from scratch
		\item it's easy to experiment as we can change the algorithm just by changing the function name to use in the \texttt{pagmo::algorithm} object creation.
	\end{enumerate}
	
	Pagmo uses a Generalized Island Model to perform computations in parallel~\cite{Izzo2012}, and because of that instead of the ``population'' the base terminology is ``archipelago''.
	An archipelago consists of ``islands'', each containing a separate population to evolve.
	
	After we run an evolution, we can visit every island and check the final result of the population evolution, including getting the best specimen, called ``champion'' in Pagmo.
	
	\subsection{Implementing the method using fuzzylite and pagmo libraries}\label{section::implementation}
	
	To implement our solver in this computational framework, first we need to declare our own ``problem'' class compatible with \texttt{pagmo::problem} class requirements.
	
	For this, in the span of this work, it is enough for us to declare the following structure with two member functions:
	
	\begin{lstlisting}[language={[11]c++}]
struct pm_problem {
	std::pair<pagmo::vector_double, pagmo::vector_double> get_bounds() const
	{
		return { {0., 0.}, {1., 1.} };
	}
	
	// Implementation of the objective function.
	pagmo::vector_double fitness(const pagmo::vector_double& dv) const
	{
		// ...implementation of the fitness function...
	}
};
	\end{lstlisting}
	
	The function \textbf{\texttt{get\_bounds}} describes the range of values from which the specimens would be generated.
	
	In the example above, we declare that every specimen is a vector of two real values between $0$ and $1$ inclusive.
	This is the setup for the ``Trivial case'' experiment from the section~\ref{trivial-case}.
	Depending on the amount of inclination values we require, we specify vectors of according length as return values of this function.
	
	The function \textbf{\texttt{fitness}} calculates the fitness value of the given specimen.
	In a classical evolutionary optimization problems, this fitness function is the pure mathematical function easy to calculate, so the algorithm itself presents the main computational challenge for the computing device.
	In our problem, however, this function contains the implementation of the control loop described in the section~\ref{fitness}.
	
	\begin{lstlisting}[language=c++]
pagmo::vector_double fitness(const pagmo::vector_double& dv) const
{
	// Inclinations is an alias for std::tuple<double, double>
	const Inclinations specimen{ dv[0], dv[1] };
	
	auto engine = init();
	
	return { simulate(specimen, engine.get())};
}
	\end{lstlisting}
	
	This function does three things:
	
	\begin{enumerate}
		\item converts the specimen from the raw data provided by Pagmo to the vector of inclinations we use in our fuzzy controller. For simplicity the mapping is direct: values of genes and values of inclinations both belong to $[0, 1] \in \mathbb{R}$.
		\item initializes the fuzzy controller
		\item runs the full simulation
	\end{enumerate}
	
	Fuzzy controller initialization is a by-the-book copy of the code from fuzzylite documentation.
	
	\begin{lstlisting}[language=c++]
std::unique_ptr<fl::Engine> init()
{
	// Initialize the engine
	std::string path{ "C:\\projects\\pm_solver\\Trivial.fll" };
	std::unique_ptr<fl::Engine> engine{ fl::FllImporter().fromFile(path) };
	
	// Checking for errors in the engine loading.
	std::string status;
	if (not engine->isReady(&status))
	{
		throw fl::Exception("[engine error] engine is not ready: \n" + status);
	}
	
	return engine;
}
	\end{lstlisting}
	
	The main point is that the specification of input and output variables and fuzzy rules is kept as a separate text file loaded at runtime.
	
	The simulation is an implementation of the control loop from the section~\ref{loop:evaluate}
	
	\begin{lstlisting}[language=c++]
double simulate(const Inclinations& inclinations, fl::Engine* engine)
{
	// Initialize a character
	Stats stats{ 0.0, 0.0, 0.0, 0.0 };
	
	// Set inclinations
	engine->getInputVariable("PhysicalInclination")
		  ->setValue(std::get<0>(inclinations));
	engine->getInputVariable("MentalInclination")
	      ->setValue(std::get<1>(inclinations));
	
	for (int i = 0; i < T; ++i)
	{
		single_step(stats, engine);
	}
	
	return fitness(stats);
}
	\end{lstlisting}
	
	We prepare the character with the starting characteristics, load the fuzzy controller with the values of inclinations and then repeat the action choice and application $T$ times.
	
	\begin{lstlisting}[language=c++]
void single_step(Stats& stats, fl::Engine* engine)
{
	// Choose an action based on the current stats and inclinations
	std::string chosen_action_name = choose_action(engine, stats);

	// Apply the effects of the chosen action
	Stats stats_diff = actions.at(chosen_action_name);
	// sum_stats is just a helper for summing two std::tuple values
	stats = sum_stats(stats, stats_diff);
}
	\end{lstlisting}
	
	This function for simplicity of implementation references the statically created global dictionary \textbf{\texttt{actions}} which maps action names to the changes in characteristics.
	With this approach once we know the name of the chosen action we can extract the characteristics changes vector and apply it to the current character state via summation.
	
	Below is an example declaration of \textbf{\texttt{actions}} from the section~\ref{trivial-case}.
	Four real values bound to each action name are changes in the four characteristics of the current character (changes in the current state of the agent).
	
	\begin{lstlisting}[language=c++]
const std::unordered_map<
	std::string, // job name
	Stats // stat changes after taking this action
> actions{
	{ "Hunting",    { 0.00, 0.01, 0.00, -0.01 } },
	{ "Lumberjack", { 0.02, 0.00, 0.00, -0.02 } },
	{ "ScienceClass", { 0.0, 0.0, 0.02, 0.0 } },
	{ "MannersClass", { 0.0, 0.0, 0.0, 0.02 } },
};	
	\end{lstlisting}
	
	An implementation of the \textbf{\texttt{choose\_action}} is very technical but it boils down to just two things:
	
	\begin{enumerate}
		\item run the fuzzy controller loaded with the current values of stats and inclinations
		\item figure out what output variable got the highest defuzzified value.
	\end{enumerate}
	
	\begin{lstlisting}[language=c++]
std::string choose_action(fl::Engine* engine, const Stats& stats)
{
	// Load the specimen into the engine
	// assume that inclinations are already set
	
	engine->getInputVariable("strength")->setValue(std::get<0>(stats));
	engine->getInputVariable("constitution")->setValue(std::get<1>(stats));
	engine->getInputVariable("intelligence")->setValue(std::get<2>(stats));
	engine->getInputVariable("refinement")->setValue(std::get<3>(stats));
	
	// Get action priorities
	engine->process();
	
	const auto& output_vars = engine->outputVariables();
	auto it = std::max_element(
	output_vars.begin(), output_vars.end(),
		[](const auto* a, const auto* b) {
			// defaulting to 0 if the value is NaN
			const auto left_priority = std::isnan(a->getValue()) 
				? 0.0 
				: a->getValue();
			const auto right_priority = std::isnan(b->getValue()) 
				? 0.0 
				: b->getValue();
			
			return left_priority < right_priority;
		}
	);
	
	// Either return the name of the action with the highest priority,
	// or the first action if no rules fired (i.e., all priorities are 0).
	std::string chosen_action_name = (it != output_vars.end())
		? (*it)->getName()
		: (*output_vars.begin())->getName();
	
	return chosen_action_name;
}
	\end{lstlisting}

	In the listing, we are setting four characteristics --- this is the setup for the trivial case from the section~\ref{trivial-case}.
		
	After we finish the simulation, we calculate the fitness.
	Below is an example from the trivial case, where the goal state is just for one of the characteristics to become higher than the threshold value $0.05$.
	
	\begin{lstlisting}[language=c++]
/** the lower the better (conforming to pagmo2 conventions) */
double fitness(const Stats& stats)
{
	// demo fitness: desirable refinement is 0.05+
	return 0.05 - std::get<3>(stats);
}
	\end{lstlisting}

	This completes the full code for the solver.
	
	\subsection{Using the implemented solver}
	
	Internal beauty of the code and external configurability were not the goals of the implementation, so to prepare the solver for the given problem a set of changes has to be done to the code itself.
	
	According to the definitions in~\ref{problem-statement}, to specify the problem we need to specify the following parameters:
	
	\begin{enumerate}
		\item a list of $n$ numerical characteristics
		\item a list $A$ of $m$ possible actions
		\item a list of changes for characteristics for each action
		\item a fitness function $\Phi$ mapping the characteristics to a single real number, representing the goal state
		\item a planning horizon $T$
	\end{enumerate}
	
	All these items are hardcoded in the implementation, directly as types and constants in the C++ source code.
	
	In addition to that, for the method explored in this work to work we need to specify the following:
	
	\begin{enumerate}
		\item a list of $q$ inclinations
		\item fuzzy variables for all the inclinations and characteristics, with all their fuzzy terms
		\item fuzzy variables which represent the priorities of each action, with all their fuzzy terms
		\item fuzzy rules binding the inclinations, characteristics and action priorities
	\end{enumerate}

	The shape of the vector of inclinations is being specified as a type in the source code, but all the fuzzy variables and rules are expressed in a DSL of fuzzylite in a separate file.
	
	The configuration for the fuzzy controller, specifically, the membership functions for the fuzzy terms, aggregation, defuzzification, conjuction, disjunction, implication operators, can be seen as hyperparameters for the solver itself detached from the particular problem instance to solve.
	
	Finally, we have an option to choose from the built-in evolutionary algorithms in pagmo library and a configuration of the archipelago of populations, which are also part of the hyperparameters.
	
	Having corrected the code to specify all the above settings, the code for solver just compiles and runs as an executable without any arguments.
	
	\section{Experiment 1: Trivial case}\label{trivial-case}

	First case has been used solely for assessing the possibility of constructing the system at all.
	It is too small to be a useful example of problems solvable by the proposed solver.
	
	We define 4 numerical attributes: \texttt{Strength}, \texttt{Constitution}, \texttt{Intelligence} and \texttt{Refinement}.
	
	These 4 attributes are changed by 4 mutually exclusive actions, listed in the table~\ref{table::trivial-case-actions}.
	
\begin{table}[h!]
	\centering
	\caption{Trivial case actions}
	\label{table::trivial-case-actions}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		              &   \multicolumn{4}{|c|}{Atrribute changes}               \\ \hline
		Action        & Strength & Constitution &  Intelligence  & Refinement   \\ \hline
		Hunting       & 0.00     & 0.01         &  0.00          & -0.01        \\ \hline
		Lumberjack    & 0.02     & 0.00         &  0.00          & -0.02        \\ \hline
		ScienceClass  & 0.00     & 0.00         &  0.02          &  0.00        \\ \hline
		MannersClass  & 0.00     & 0.00         &  0.00          &  0.02        \\ \hline
	\end{tabular}
\end{table}

	We will run this problem on a goal states reachable in 4 steps, meaning, our $T$ is 4 for a trivial case.

	This scenario represents a trivial case, with only $4^{3}$ possible action sequences—a total of 64.
	The small state space allows for exhaustive enumeration and manual verification of results.
	This case serves to validate the correctness of the implementation and the fuzzy controller, as the system's behavior can be easily traced and analyzed by hand.

	From the list of attributes and actions we synthesize two inclinations: \texttt{Physical Inclination} and \texttt{Mental Inclination}, which represent, correspondingly, ``an inclination to perform actions related to physical attributes improvement'' and ``an inclination to perform actions related to mental attributes improvement''.
	
	The full text of the fuzzy controller for the trivial case, which binds two inclinations and four actions, looks as follows:
	
	\begin{lstlisting}[language=yaml]
Engine: Trivial

# Inclinations

InputVariable: PhysicalInclination
	enabled: true
	range: 0 1.000
	lock-range: false
	term: tiny Ramp 0.330 0.000 
	term: low Triangle 0.000 0.330 0.670
	term: high Triangle 0.330 0.670 1.000
	term: highest Ramp 0.670 1.000

InputVariable: MentalInclination
	enabled: true
	range: 0 1.000
	lock-range: false
	term: tiny Ramp 0.330 0.000 
	term: low Triangle 0.000 0.330 0.670
	term: high Triangle 0.330 0.670 1.000
	term: highest Ramp 0.670 1.000

# Action Priorities

OutputVariable: Hunting
	enabled: true
	range: 0.000 1.000
	lock-range: false
	aggregation: Maximum
	defuzzifier: Centroid 100
	default: nan
	lock-previous: false
	term: low Ramp 1.000 0.000
	term: high Ramp 0.000 1.000

OutputVariable: Lumberjack
	enabled: true
	range: 0.000 1.000
	lock-range: false
	aggregation: Maximum
	defuzzifier: Centroid 100
	default: nan
	lock-previous: false
	term: low Ramp 1.000 0.000
	term: high Ramp 0.000 1.000

OutputVariable: ScienceClass
	enabled: true
	range: 0.000 1.000
	lock-range: false
	aggregation: Maximum
	defuzzifier: Centroid 100
	default: nan
	lock-previous: false
	term: low Ramp 1.000 0.000
	term: high Ramp 0.000 1.000

OutputVariable: MannersClass
	enabled: true
	range: 0.000 1.000
	lock-range: false
	aggregation: Maximum
	defuzzifier: Centroid 100
	default: nan
	lock-previous: false
	term: low Ramp 1.000 0.000
	term: high Ramp 0.000 1.000


# Rules

RuleBlock: mamdani
	enabled: true
	conjunction: Minimum
	disjunction: Maximum
	implication: AlgebraicProduct
	activation: General
	rule: if PhysicalInclination is low  and MentalInclination is low    then MannersClass is high
	rule: if PhysicalInclination is low  and MentalInclination is low    then Hunting is low
	rule: if MentalInclination is high   and PhysicalInclination is low  then ScienceClass is high
	rule: if MentalInclination is high   and PhysicalInclination is low  then Lumberjack is low
	rule: if PhysicalInclination is high and MentalInclination is low    then Lumberjack is high
	rule: if PhysicalInclination is high and MentalInclination is low    then ScienceClass is low
	rule: if MentalInclination is high   and PhysicalInclination is high then Hunting is high
	rule: if MentalInclination is high   and PhysicalInclination is high then MannersClass is low
	\end{lstlisting}
	
	Correspondingly, the goal state is expressed as the following fitness function:
	
	\begin{lstlisting}
double fitness(const Stats& stats)
{
	// Stat of index 3 is refinement
	return 0.07 - std::get<3>(stats);
}
	\end{lstlisting}
	
	In the 4-tuple \texttt{Stats} the element at index 3 (zero-based) is a Refinement attribute, so this fitness function expresses the goal ``Refinement must be more than 0.07''.
	
	Given the table of action effects and the goal, it's obvious that the only correct sequence of actions which can reach this goal is an action ``MannersClass'' repeated 4 times in a row, as this action is the only way to get an increase in the ``Refinement'' attribute.
	
	The archipelago has been configured in the following manner:
	
	\begin{lstlisting}
pagmo::algorithm algo(pagmo::sade(100));
pagmo::archipelago archi(16u, algo, prob, 20u);
	\end{lstlisting}

	Four arguments to the \texttt{pagmo::archipelago} constructor are number of islands, algorithm to use, problem to solve and size of the population on each island.
	These are the slice of the hyperparameters which are related to evolutionary optimizations.
	
	The algorithm that has been chosen (\texttt{pagmo::sade}) is an instance of Self-adaptive Differential Evolution algorithm, jDE variant~\cite{sade_jDE}~\cite{sade_iDE}, for no reason other than being mentioned the first in the Pagmo documentation examples.

	Running this system with the above setup results in 16 islands all evolving to the specimen which successfully reach the goal.
	
	The following listing is an example listing of champions across the archipelago --- due to stochastic nature of evolutionary optimization, every run will provide different specimen.
	
	\begin{verbatim}
		{0.292813, 0.31037}   fitness -0.03
		{0.223563, 0.274446}  fitness -0.03
		{0.667839, 0.0552651} fitness -0.03
		... 12 more ...
		{0.036359, 0.0444171} fitness -0.03
	\end{verbatim}
	
	Due to the deterministic nature of the problem, we can get the exact list of actions from the specimen by simply calling the \texttt{simulate} function (explained in the section~\ref{section::implementation}) with the inclination values gotten from the archipelago champions.
	
	In this case, the list of actions is correctly inferred as 4 instances of ``MannersClass''.
	
	If we change the goal to check the ``Constitution'', then the system converges to an action sequence of 4 instances of a ``Hunting'' action, with the following specimen examples:
	
	\begin{verbatim}
		{0.379914, 0.929484} fitness 0.01
		{0.334496, 0.832509} fitness 0.01
		{0.96071, 0.470057}  fitness 0.01
		... 12 more ...
		{0.516274, 0.708208} fitness 0.01
	\end{verbatim}
	
	
	\subsection{Discussion of the results}
		
	This basic trivial case confirms that we are indeed able to construct the hybrid fuzzy-genetic system which is able to produce optimal sequences of actions which lead to the goal state.
		
	Using the fuzzy controller came out more complicated than it could be seen from the theory alone.
	While the target of the work was reducing the search state space, the amount of parameters in the fuzzy controller exploded the hyperparameters space instead, as by different configuration of the fuzzy rules and fuzzy variables we can change the behavior of the specimen.
	
	It can be seen that if we exclude the current state of the specimen from the fuzzy rules, we trivialize the trajectories, reducing them to repetition of the same action $T$ times.
	While this does not simplify the optimization step, as it is assumed that though (\ref{q<<n}), $q$ is still large enough for the bruteforce enumeration to be intractable, it leaves us with action sequences intuitively unfit as solutions for any realistic nontrivial goal states.
	
	If we setup the fuzzy action-prioritizing rules in such a way that they would indeed use the current state of the specimen, we do turn the problem into the control one with non-trivial solutions, but at the same time we end up having to specify not only at least one rule per each action priority, but also at least one rule per each \textit{term} per \textit{each input variable}, which starts competing with the complexity of the problem we are trying to solve by this method in the first place.
	
	In the discussion of reducing the dimensionality of the problem, one should not forget the actual issue which explodes the dimensionality of the problem.
	While on the surface the method explored in this work is based on the inequality (\ref{q<<n}) and the amount of inclinations seems to be the target of discussions, the actual solution to the origin problem is a \textit{list of actions}, and the true reason for dimensionality explosion is the length $T$ of the list of actions to find and the amount of actions to be considered at each step.
	
	It also can be seen that the configuration of the fuzzy controller and a tiebreaking rule is paramount for getting the solution.
	Intuitively by construction it can be expected that, if we set some ``Physical'' attribute as a goal, we expect that the specimen with high ``Physical Inclination'' and low ``Mental Inclination'' will be the only possible solution, but it's not the case.
	
	In the example results for the ``Strength must be higher than 0.07'' case above, we can see a winning specimen with ``Physical Inclination'' being as low as 0.379914 and ``Mental Inclination'' being as high as 0.929484,
	which is a situation completely opposite to the expectations.
	If we single-step the evolution process for this specimen, we can see that on the step of choosing an action it gets actions with identical priorities, but opposite effects.
	
	For a visual example, this is how the text report looks like if we introduce it in the program appropriately:
	
	\begin{verbatim}
island champion: {0.379914, 0.929484}
Step 1:
Comparing Hunting with value 0.66665 and Lumberjack with value 0.33335
Comparing Hunting with value 0.66665 and ScienceClass with value 0.66665
Comparing Hunting with value 0.66665 and MannersClass with value 0.33335
Chosen action: Hunting
	\end{verbatim}
	
	Three following steps are not shown because they are identical to this one.
	We can see that priorities of the ``Hunting'' and ``ScienceClass'' actions were inferred as identical,
	and the ``Hunting'' action has been chosen solely by the reason of being the first in the list of actions compared to ``ScienceClass''.
	
	From one perspective we can see it as a deficiency in an algorithm and implement a more robust tiebreaker, but on the other hand we should not forget that the solution we seek is the list of actions leading to the goal state,
	not the inclination values which are essentially transient intermediaries representing paths in the actions space.
	
	\section{Experiment 2: Base control case}
    
	24 characteristics, 25 actions, 100 steps.

	This case is the base case, as it introduces enough complexity to test the proposed approach and at the same time compare it with classical approaches.

	A decision tree of the size $12^{100}$ is already too large to be completely enumerated.

	In this case we copy all the numerical attributes of the character sans one from the original Princess Maker 2 problem, and all the actions with their effects on these attributes.
	
	The planning horizon of 100 steps is long enough to be non-trivial, but does not pose significant challenges for standard RL algorithms.

	Imitating the full \textit{Princess Maker 2} run requires 1200 steps (``days'') which will be done in Experiment 3.
	
	% TODO: run this case with various T and plot a graph T versus runtime.
	% prove that it's polynomial!
	
	\subsection{Discussion of the results}
		(to be done)
		
	\section{Experiment 3: Origin case}

	The complete Princess Maker 2 case is a problem with 50 numeric characteristics of a character and 25 actions to choose from, with a planning horizon of 360 steps.

	This case is an attempt to directly solve the original problem which started this work.
	It will be used as a benchmark for the proposed solution on a real-world problem.

	In this case we increase not only the purely numerical characteristics of the problem, but its complexity itself.

First, we will introduce two characteristics which were the core of the original \textit{Princess Maker} problem: money and stress.
These characteristics are resources which the player must manage effectively while scheduling the activity of the character.
Every action increases the stress.
Some actions increase money --- they are called ``jobs''.
Some actions decrease money --- they are called ``lessons''.

Actions which decrease money become unavailable when there's not enough money.
When the stress reaches some boundary values, in the original game a series of events of increasing severity happen.
In this work we'll not model all this complexity.
Instead we'll maintain some boundary value of stress after which no actions would be selected except the ones decreasing it.


Second, we will try modeling these restrictions completely as fuzzy rules.
In this case, the fuzzy controller would be thought of as ``predicting'' the problems instead of hitting them and backtracking.

In the original game effectiveness of the actions dependent on the current values of the attributes of the character.
In addition to that, some of the actions --- jobs --- become available only starting from particular age, meaning, they should be unavailable until we reach a particular step number in the evolution of a single specimen.

We will not implement these mechanics in the span of this work.

	Managing money is a significant trick in the existing strategies for completing the actual \textit{The Princess Maker 2} game.
While the full real walkthrough for reaching the Queen ending relies heavily on the narrative events in the game which arbitrarily change the characteristics depending on the event itself, separately from the mechanic of scheduling the activities, the overall strategy is preparing the exact amount of money, pre-calculated beforehand, in the beginning, to use them in one go on all the required lessons and nothing else.
Fuzzy controller will not be able to replicate this behavior exactly, but the concept of ``inclinations'' is based on a hypothesis that we can emulate it by finding a character ``inclined'' in such a way to behave like that.

	(to be done)
	
		\subsection{Discussion of the results}
		(to be done)

% The first one is regarding the data. In the documentation you talk about the game, but you don’t explain the information about the input variables, you should give more details to get the complexity on how much information there is in the input. You also talk about the chromosomes, but I cannot see the translation from the input into these chromosomes.
%I also would like to know how you want to evaluate the quality of the ouptut.
%Are these experiments that you explain in the video described in the report?

	
	\section{Conclusions and Future Work}

	First, let us make a conclusion tangential to the origin problem but related to the chosen theoretical toolset.
	While the fuzzy controller can indeed be seen as a tool to formalize the decision making using the expert knowledge in the given domain, it is too complicated mechanism by itself to help reducing the complexity of the problem it's solving.
	One should treat it not as a tool which helps make complicated problems simpler but which, hopefully, makes unsolvable ones solvable at all, as proper configuration of the fuzzy controller is already a problem in itself.

	Despite the context of the problem being a computer game, the problem itself is a general one, and the proposed approach can be applied to any high-dimensional deterministic planning problem.
	This constitutes the core value of this work.

	In the span of this work, only three distinct cases were explored, and the more thorough exploration of the parameter space is left for a dissertation-level research.

	The proof-of-concept solver implemented and discussed in this work is obviously an instance of an automated planner, for which an extensive framework of both machinery and benchmarking exists.
	The logical next step would be to determine a correct place of this method in the existing body of knowledge about the automated planners and perform formal comparisons of performance with some other state-of-the-art methods.

	Another work which exploits the idea of using the domain knowledge in the solution process exists, but it uses the Linear Temporal Logic instead, and utilizes the existing PDDL automatic planning machinery~\cite{LU2025121666}.

	The paper~\cite{song2023rl_ea} briefly mentioned in the section \ref{section::evolutionary}, is an possible alternative hybrid system, where the target of evolution is a reinforcement learning process.

	\printbibliography

\end{document}
